---
title: "Reproduction of Chakraborty 2021: An intracategorical analysis of COVID-19 and people with disabilities"
author: "Joseph Holler, Junyi Zhou, Peter Kedron, Drew An-Pham, Derrick Burt"
date: "`r Sys.Date()`"
output: pdf_document
editor_options: 
  markdown: 
    wrap: sentence
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding, output_dir = "../../docs/report") })
---

Version 1.4 \| First Created Jul 7, 2021

# Abstract

Chakraborty (2021) investigates the relationships between COVID-19 rates and demographic characteristics of people with disabilities by county in the lower 48 states.
The study aims to examine public concern that persons with disabilities (PwDs) face disproportionate challenges due to COVID-19.
To investigate this, Chakraborty examines the statistical relationship between confirmed county-level COVID-19 case rates and county-level socio-demographic and disability variables.
Specifically, Chakraborty tests county-level bivariate correlations between COVID-19 incidence against the percentage of disability and socio-demographic category, with a separate hypothesis and model for each subcategory within disability, race, ethnicity, age, and biological sex.
To control for differences between states and geographic clusters of COVID-19 outbreaks, Chakraborty uses five generalized estimating equation (GEE) models to predict the relationship and significance between COVID-19 incidence and disability subgroups within each socio-demographic category while considering inter-county spatial clusters.
Chakraborty (2021) finds significant positive relationships between COVID-19 rates and socially vulnerable demographic categories of race, ethnicity, poverty, age, and biological sex.

This reproduction study is motivated by expanding the potential impact of Chakraborty's study for policy, research, and teaching purposes.
Measuring the relationship between COVID-19 incidence and socio-demographic and disability characteristics can provide important information for public health policy-making and resource allocation.
A fully reproducible study will increase the accessibility, transparency, and potential impact of Chakraborty's (2021) study by publishing a compendium complete with metadata, data, and code.
This will allow other researchers to review, extend, and modify the study and will allow students of geography and spatial epidemiology to learn from the study design and methods.

In this reproduction, we will attempt to identically reproduce all of the results from the original study.
This will include the map of county level distribution of COVID-19 incidence rates (Fig. 1), the summary statistics for disability and sociodemographic variables and bivariate correlations with county-level COVID-19 incidence rate (Table 1), and the GEE models for predicting COVID-19 county-level incidence rate (Table 2).
A successful reproduction should be able to generate identical results as published by Chakraborty (2021).

The replication study data and code will be made available in a GitHub repository to the greatest extent that licensing and file sizes permit.
The repository will be made public at [github.com/HEGSRR/RPr-Chakraborty2021]().

Chakraborty, J.
2021.
Social inequities in the distribution of COVID-19: An intra-categorical analysis of people with disabilities in the U.S.
Disability and Health Journal 14:1-5.
[DOI:[10.1016/j.dhjo.2020.101007](DOI:%5B10.1016/j.dhjo.2020.101007){.uri}]()

## Keywords

COVID-19; Disability; Intersectionality; Race/ethnicity; Poverty; Reproducibility

# Study Design

The reproduction study will attempt to implement the original study as closely as possible to reproduce the map of county level distribution of COVID-19 incidence rate, the summary statistics and bivariate correlation for disability characteristics and COVID-19 incidence, and the generalized estimating equations.
Our two confirmatory hypotheses are that we will be able to exactly reproduce Chakraborty's results as presented in table 1 and table 2 of Chakraborty (2021).
Stated as null hypotheses:

> H1: There is a less than perfect match between Chakraborty's bivariate correlation coefficient for each disability/sociodemographic variable and COVID-19 incidence rate and our bivariate correlation coefficient for each disability/sociodemographic variable and COVID-19 incidence rate.

> H2: There is a less than perfect match between Chakraborty's beta coefficient for the GEE of each disability/sociodemographic variable and our beta coefficient for the GEE of each disability/sociodemographic variable.

There are multiple models being tested within each of the two hypotheses.
That is, H1 and H2 both encompass five models, including one for each dimension of socio-demographics: race, ethnicity, poverty status, age, and biological sex.

# Original study design

The original study is **observational**, with the **exploratory** objective of determining "whether COVID-19 incidence is significantly greater in counties containing higher percentages of socio-demographically disadvantaged [people with disabilities], based on their race, ethnicity, poverty status, age, and biological sex" (Chakraborty 2021).
This exploratory objective is broken down into five implicit hypotheses that each of the demographic characteristics of people with disabilities is associated with higher COVID-19 incidence rates.

The **spatial extent** of the study are the 49 contiguous states in the U.S.
The **spatial scale** of the analysis is at the county level.
Both COVID-19 incidence rates and demographic variables are all measured at the county level.
The **temporal extent** of the COVID-19 data ranges from 1/22/2020 (when John Hopkins began collecting the data) to 8/1/2020 (when the data was retrieved for the original study).
The data on disability and sociodemographic characteristics come from the U.S.
Census American Community Survey (ACS) five-year estimates for 2018 (2014-2018).

There is no **randomization** in the original study.

![](../../docs/report/workflow.jpg "Workflow diagram")

# Computational environment

The study was originally conducted using SaTScan software (unspecified version) to implement the Kulldorff spatial scan statistic.
Other software are not specified in the publication; however data files and communication with the author show that spatial analysis and mapping was conducted in ArcGIS and generalized estimating equation (GEE) models were calculated in SPSS.

This reproduction study uses R, including the SpatialEpi package for the Kulldorff spatial scan statistics and the geepack package for GEE models.

```{r setup, message = FALSE, include = FALSE}

knitr::opts_chunk$set(echo = FALSE)

# list of required packages
packages <- c(
  "tidycensus", "tidyverse", "downloader", "sf", "classInt", "readr",
  "here", "s2", "pastecs", "tmap", "SpatialEpi", "svDialogs",
  "geepack", "knitr"
)

# load and install required packages
package.check <- lapply(
  packages,
  FUN = function(x) {
    if (!require(x, character.only = TRUE)) {
      install.packages(x, dependencies = TRUE, quietly = TRUE)
      library(x, character.only = TRUE)
    }
  }
)

# save the R processing environment
writeLines(
  capture.output(sessionInfo()),
  here("procedure", "environment", paste0("r-environment-", Sys.Date(), ".txt"))
)
```

# Data

## American Community Survey

American Community Survey (ACS) data for sociodemographic subcategories of people with disabilities can be accessed by using the `tidycensus` package to query the Census API. This requires an API key which can be acquired at [api.census.gov/data/key_signup.html](https://api.census.gov/data/key_signup.html)

```{r Load ACS Data, message = FALSE, eval = FALSE}

# get API Key
# we could store this in the raw/private or scratch folder and load if the
# researcher has already entered it once
census_api_key(dlgInput(
  "Enter a Census API Key",
  Sys.getenv("CENSUS_API_KEY")
)$res,
overwrite = TRUE
)

# Query disability demographic data with geographic boundaries
acs <- get_acs(
  geography = "county",
  table = "S1810",
  year = 2018,
  output = "wide",
  cache_table = TRUE,
  geometry = TRUE,
  keep_geo_vars = TRUE
)

# Query poverty and disability data
acs_pov <- get_acs(
  geography = "county",
  table = "C18130",
  year = 2018,
  output = "wide",
  cache_table = TRUE
)

# Query state geographic data
state <- get_acs(geography = "state", 
                 year = 2018, 
                 variables = c("B01001_001"),
                 geometry = TRUE,
                 keep_geo_vars = TRUE)
```

The original study extent is the lower 48 states and Washington D.C. Therefore, Alaska, Hawai'i and Puerto Rico are removed from the data (workflow diagram step 1).
Data on people with disabilities in poverty is derived from a different census table (C18130) than data on people with disabilities and age, race, ethnicity, age, and biological sex (S1810).
Therefore, join the poverty data to the other data using the GEOID (workflow diagram step 3).

```{r filter and join acs data, message = FALSE, eval = FALSE}

# Remove Alaska, Hawaii & Puerto Rico
acs <- filter(acs, !STATEFP %in% c("02", "15", "72"))
state <- filter(state, !STATEFP %in% c("02", "15", "72"))

# Join poverty data to disability data
acs <- left_join(acs, acs_pov, by = "GEOID")
rm(acs_pov)

```

Save the raw ACS data to `data/raw/public/acs.gpkg`

```{r save ACS data, message = F, eval = FALSE}
# Save downloaded acs data to acs.gpkg
write_sf(
    acs,
    here("data", "raw", "public", "acs.gpkg"),
    layer = "acs")
write_sf(
    state,
    here("data", "raw", "public", "acs.gpkg"),
    layer = "state"
)
```

Load the raw ACS data.
Optionally, begin processing here.

```{r load ACS data, message = F}
acs <- read_sf(here("data", "raw", "public", "acs.gpkg"), layer = "acs")
state <- read_sf(here("data", "raw", "public", "acs.gpkg"), layer = "state")
```

Calculate independent socio-demographic variables of people with disabilities as percentages for each sub-category of disability (race, ethnicity, poverty, age, and biological sex) and remove raw census data from the data frame (workflow diagram step 4).
Reproject the data into an Albers equal area conic projection.

```{r Preprocess ACS data, message = FALSE}
# calculate percentages
acs_derived <- mutate(acs,
  dis_pct = S1810_C02_001E / S1810_C01_001E * 100,
  white_pct = S1810_C02_004E / S1810_C01_001E * 100,
  black_pct = S1810_C02_005E / S1810_C01_001E * 100,
  native_pct = S1810_C02_006E / S1810_C01_001E * 100,
  asian_pct = S1810_C02_007E / S1810_C01_001E * 100,
  other_pct =
    (S1810_C02_008E + S1810_C02_009E + S1810_C02_010E) / S1810_C01_001E * 100,
  non_hisp_white_pct = S1810_C02_011E / S1810_C01_001E * 100,
  hisp_pct = S1810_C02_012E / S1810_C01_001E * 100,
  non_hisp_non_white_pct =
    (S1810_C02_001E - S1810_C02_012E - S1810_C02_011E) / S1810_C01_001E * 100,
  bpov_pct = (C18130_004E + C18130_011E + C18130_018E) / C18130_001E * 100,
  apov_pct = (C18130_005E + C18130_012E + C18130_019E) / C18130_001E * 100,
  pct_5_17 = S1810_C02_014E / S1810_C01_001E * 100,
  pct_18_34 = S1810_C02_015E / S1810_C01_001E * 100,
  pct_35_64 = S1810_C02_016E / S1810_C01_001E * 100,
  pct_65_74 = S1810_C02_017E / S1810_C01_001E * 100,
  pct_75 = S1810_C02_018E / S1810_C01_001E * 100,
  male_pct = S1810_C02_002E / S1810_C01_001E * 100,
  female_pct = S1810_C02_003E / S1810_C01_001E * 100
)

# select only relevant geographic identifiers and derived percentages
# and transform to USA Contiguous Albers Equal Area Conic projection
acs_derived <- acs_derived %>%
  select(
    geoid = GEOID,
    statefp = STATEFP,
    county = NAME.x,
    county_st = NAME,
    contains("pct")
  ) %>%
  st_transform(5070) %>% 
  st_make_valid()
```

## COVID-19 rates

Data on COVID-19 rates from the Johns Hopkins University dashboard have been provided directly with the research compendium because the data is no longer available online in the state in which it was downloaded on August 1, 2020.
The dashboard and cumulative counts of COVID-19 cases and deaths were continually updated, so an exact reproduction required communication with the original author, Jayajit Chakraborty, for assistance with provision of data from August 1, 2020.

```{r load covid data}
covid <- read_sf(here("data", "raw", "public", "covidcase080120.gpkg"))
covid <- select(covid,
  fips = FIPS,
  pop = POP_ESTIMA,
  cases = Confirmed,
  x = X, y = Y
)
```

Calculate the COVID incidence rate as the cases per 100,000 people (workflow step 2).
*Unplanned Deviation* Initially the descriptive statistics for the COVID rate were slightly different,
but we have resolved the difference by rounding the rate to 0 decimal places.

```{r covid rate}
covid$covid_rate <- round(covid$cases / covid$pop * 100000, 2)
```

Join dependent COVID data to independent ACS sociodemographic data.

```{r join covid to acs}
# create a non-spatial table of COVID data for use in joining and cluster analysis
covid_table <- st_drop_geometry(covid)

# Join COVID data to acs data
acs_covid <- left_join(acs_derived,
  select(covid_table, fips, covid_rate),
  by = c("geoid" = "fips")
)

# move covid_rate column prior to disability percentages
acs_covid <- select(
  acs_covid, geoid, statefp, county, county_st, covid_rate,
  everything()
)
```

## Missing Data

There is one county with missing disability and poverty data.
This was not mentioned in the original study or our pre-analyis plan.
However, we replace the missing data with zeros, producing results identical to Chakraborty's.

```{r missing data}
# county with missing data
filter(acs_covid, is.na(bpov_pct)) %>% st_drop_geometry()

# replace NA with 0 for missing data
acs_covid[is.na(acs_covid$bpov_pct), ]$bpov_pct <- 0
acs_covid[is.na(acs_covid$apov_pct), ]$apov_pct <- 0
```

## Map COVID-19 Incidence

Map the county level distribution of COVID-19 incidence rates, comparing to Figure 1 of the original study.

```{r map covid rates, message = FALSE}
tm1 <- tm_shape(acs_covid) +
  tm_polygons("covid_rate",
    title = "COVID-19 Cases per 100,000\n(01/22/2020 ~ 08/01/2020)",
    style = "quantile",
    border.alpha = .2,
    lwd = 0.2,
    palette = "YlOrBr",
  ) +
  tm_shape(state) +
  tm_borders("grey", lwd = .5) +
  tm_layout(
    legend.position = c("left", "bottom"),
    legend.title.size = 0.8,
    legend.text.size = 0.5
  )

tm1

tmap_save(tm1, here("results", "figures", "covid_rates.png"))
```

## Map Disability

**Planned deviation**: We also map the spatial distribution of the percent of
people with any disability.

```{r map disability rates, message = FALSE}
tm2 <- tm_shape(acs_covid) +
  tm_polygons("dis_pct",
    title = "Percent with Disability\n(ACS 2014-2018)",
    style = "quantile",
    border.alpha = .2,
    lwd = 0.2,
    palette = "YlOrBr"
  ) +
  tm_shape(state) +
  tm_borders("grey", lwd = .5) +
  tm_layout(
  legend.position = c("left", "bottom"),
  legend.title.size = 0.8,
  legend.text.size = 0.5)

tm2

tmap_save(tm2, here("results", "figures", "disability_rates.png"))
```

## Descriptive Statistics

Calculate descriptive statistics for dependent covid rate and independent socio-demographic characteristics, reproducing the min, max, mean, and SD columns of original study table 1.

**Planned deviation**: We also calculate the Shapiro Wilk test for normality.

```{r descriptive statistics}
acs_covid_stats <- acs_covid %>%
  st_drop_geometry() %>%
  select(where(is.numeric)) %>%
  stat.desc(norm = TRUE) %>%
  round(2) %>%
  t() %>%
  as.data.frame() %>%
  select(min, max, mean, SD = std.dev, ShapiroWilk = normtest.W, p = normtest.p)

acs_covid_stats %>% kable()
```

Compare reproduced descriptive statistics to original descriptive statistics.
Difference is calculated as 'reproduction study - original study'
Identical results will result in zero.

```{r compare descriptive stats}
# load original table 1 results
table1 <- read.csv(here("data", "raw", "public", "chakraborty", "table1.csv"))
# subtract original results from reproduced results
(select(acs_covid_stats, min, max, mean, SD) - 
  select(table1, min, max, mean, SD)) %>% 
  kable()
```
The descriptive statistics are identical, except that the original study seems to have rounded the COVID-19 statistics to zero decimal places.

# Analytical Methods

## Bivariate Parametric Correlation Analysis

Calculate Pearson's R Correlation Coefficient of each independent variable and the COVID-19 incidence rate, reproducing the Pearson's R column of the original study Table 1.

```{r pearsons correlation}
df <- sum(!is.na(acs_covid$dis_pct)) - 2

pearsons_r <- acs_covid %>%
  select(where(is.numeric)) %>%
  st_drop_geometry() %>%
  cor(method = "pearson", use = "everything") %>%
  as.data.frame() %>%
  select(r = covid_rate) %>%
  mutate(
    t = abs(r) / sqrt((1 - r^2) / (df)),
    p = pt(t, df, lower.tail = FALSE)
  ) %>%
  round(3) %>%
  rownames_to_column("variable") %>%
  filter(variable != "covid_rate")

pearsons_r %>% kable()
```

Compare the reproduced Pearson's *r* correlation coefficients to the original study's Pearson's *r* correlation coefficients.
Stars indicates the significance level with two stars for `p < 0.01` and one star for `p < 0.05`.
 is calculated as `reproduction study - original study`
Direction difference is calculated as `(reproduction > 0) - (original > 0)`, giving 0 if both coefficients have the same direction, 1 if the reproduction is positive and the original is negative, and -1 if the reproduction is negative but the original is positive.

```{r compare pearsons correlation}
# calculate number of significance stars at p<0.01 and P<0.05 levels.
pearsons_r <- mutate(pearsons_r, rp_stars = as.numeric(as.character(cut(p,
  breaks = c(-0.1, 0.01, 0.05, 1),
  labels = c(2, 1, 0)
))))

correlations <- table1 %>% filter(variable != "covid_rate") %>% 
  select(variable,  or_r = r, or_stars = stars) %>% 
  left_join(select(pearsons_r, variable, rp_r = r, rp_stars), by = "variable") 

correlations <- correlations %>% bind_cols(rename_with(correlations[,4:5] - correlations[,2:3],
                                                      ~ paste0(.x, "_diff")))

correlations <- correlations %>% mutate(rp_dir_diff = (rp_r > 0) - (or_r > 0))

correlations %>% kable()
```

All but one Pearson's correlation coefficient was significant to the same level, and the exception was age 18 to 34.
All of the coefficients had the same direction.
The reproduced Pearson's *r* coefficients were inexact but very similar to the original study, falling within a range of +/-0.006.

**Unplanned Deviation**: We should expect identical results for this correlation test, so we loaded the original author's data from `Aug1GEEdata.csv` to re-test the statistic, calculated as `unplanned_r` below.

```{r original data pearson correlation}
original_gee <- read.csv(here("data", "raw", "public", "chakraborty", "Aug1GEEdata.csv"))
original_gee %>%
  select(Incidence, PerDisable, starts_with("PD")) %>% 
  cor(method = "pearson", use = "everything") %>%
  as.data.frame()  %>%
  rownames_to_column("or_variable") %>%
  filter(or_variable != "Incidence")  %>%
  select(or_variable, unplanned_r = Incidence) %>% 
  bind_cols(correlations[,1:2]) %>% 
  mutate(unplanned_r = round(unplanned_r, 3), diff = unplanned_r - or_r) %>% 
  select(variable, unplanned_r, or_r, diff) %>% 
  kable()
```

The author's original data produced coefficients identical to the original publication!
Considering the bitwise reproduction of descriptive statistics along with the accurate recalculation of correlation statistics from original data, could it be possible that the data values are correct but have been reassigned / transposed to different counties? 

*Unplanned Deviation*: We re-calculate the COVID-19 incidence rate with the original author's data and select for unequal results.

```{r compare incidence rate}
original_gee <- original_gee %>% mutate(recalc_Incidence = round(Cases / Total_POP * 100000, 2))
original_gee %>% filter(recalc_Incidence != Incidence) %>% 
  select(COUNTY_FIPS, ST_Name, Countyname, Total_POP, Cases, Incidence, recalc_incidence) %>% kable()
```

We found that 13 counties had incorrect COVID-19 incidence scores, and the scores seem to be transposed to other counties, such that the overall descriptive statistics were accurate but the correlation coefficients were inaccurate.

## Bivariate Nonparametric Correlation Analysis

**Unplanned Deviation**: The dependent and independent variables in this study do not have normal distributions, as shown in the Shapiro-Wilk test results above.
Therefore, we deviate from the original study to use the Spearman's Rho non-parametric correlation test.

```{r spearmans correlation}
df <- sum(!is.na(acs_covid$dis_pct)) - 2

spearmans_rho <- acs_covid %>%
  select(where(is.numeric)) %>%
  st_drop_geometry() %>%
  cor(method = "spearman", use = "everything") %>%
  as.data.frame() %>%
  select(rho = covid_rate) %>%
  mutate(
    t = abs(rho) / sqrt((1 - rho^2) / (df)),
    p = pt(t, df, lower.tail = FALSE)
  ) %>%
  round(3) %>%
  rownames_to_column("variable") %>%
  filter(variable != "covid_rate")
```

Compare the Spearman's *rho* correlation coefficients to the reproduced Pearson's *r* correlation coefficients.

```{r compare spearmans correlation}
# calculate number of significance stars at p<0.01 and P<0.05 levels.
spearmans_rho <- mutate(spearmans_rho, rp_rho_stars = as.numeric(as.character(cut(p,
  breaks = c(-0.1, 0.01, 0.05, 1),
  labels = c(2, 1, 0)
))))

correlations <- correlations[,1:8] %>% 
  left_join(select(spearmans_rho, variable, rp_rho = rho, rp_rho_stars), by = "variable")

corrdiff <- select(correlations, starts_with("rp_rho")) - 
  select(correlations, rp_r, rp_stars)

correlations <- correlations %>% bind_cols(rename_with(corrdiff, ~ paste0(.x, "_diff")))
rm(corrdiff)

correlations <- correlations %>% mutate(rp_rho_dir_diff = (rp_rho > 0) - (rp_r > 0))

correlations %>% select(variable, rp_r, rp_stars, starts_with("rp_rho")) %>%  kable()
```

Three variables change significance levels, with Native American and Other races gaining significance and age 18-34 losing significance. 
Two correlations change direction, with both Native American race and Female households switching from positive correlations to negative correlations. 
Instabilities between the parametric and non-parametic correlations reflect variables with very skewed distributions and/or weak correlations at the county level.
In such distributions, the tails and outliers have more weight in parametric tests than in non-parametric tests.
This is illustrated with a scatterplot and linear regression below.

```{r plot bivariate}
plot(acs_covid$native_pct, 
     acs_covid$covid_rate, 
     xlab="Percent Native American",
     ylab="COVID-19 Incidence",
     pch=16,
     col = rgb(0,0,0,0.1))
lines(abline(lm(acs_covid$covid_rate ~ acs_covid$native_pct)))
```

## Kulldorf Spatial Scan Cluster Detection

This accomplishes the step 6 of the workflow diagram

Note that the statistic is a Monte Carlo simulation with 999 iterations.
Therefore, if you wish to exactly reproduce the same results as our reproduction attempt, please **do not run this section**.
Instead, load the scan results below.
This code block can take more than 10-20 minutes to run.

```{r SpatialEpi Kulldorff spatial scan, eval = FALSE}
covid_geo <- covid_table %>%
  select(x, y) %>%
  latlong2grid()
# latlong2grid approximates an equidistant grid measured in kilometers
# need to look more into the methods of this, but it surely is not as good
# as a geodesic calculation. SaTScan uses spherical or ellipsoidal distance
# Latitude is multiplied by 111.133 and Longitude is multiplied by 86.97357

# calculate expected cases with one strata
expected.cases <- expected(covid_table$pop, covid_table$cases, 1)

# Kulldorff spatial scan statistic
covid_kulldorff <- kulldorff(
  geo = covid_geo,
  cases = covid_table$cases,
  population = covid_table$pop,
  expected.cases = expected.cases,
  pop.upper.bound = 0.5,
  n.simulations = 999,
  alpha.level = 0.05,
  plot = TRUE
)

rm(covid_table, covid_geo, expected.cases)
```

## Save scan results

```{r save spatial scan results, eval = FALSE}

saveRDS(covid_kulldorff,
  file = here("data", "derived", "public", "covid_kulldorff.RDS")
)

```

## Optionally, load scan results

```{r load spatial scan results}

covid_kulldorff <- readRDS(
  here("data", "derived", "public", "covid_kulldorff.RDS")
)

print("Most likely cluster:")

covid_kulldorff$most.likely.cluster

print(paste0("Secondary clusters: ", length(covid_kulldorff$secondary.clusters)))

```

## Summarize spatial scan clusters by county

This accomplishes the step 7 and 8 of the workflow diagram

Summarize the results of the Kulldorff spatial scan cluster detection by cluster

```{r summarize Kulldorff results by cluster}

# Get a list of primary clusters
primary <- covid_kulldorff$most.likely.cluster$location.IDs.included
cluster_risk <- covid_table[primary, c(1, 2, 3)] # extract countyID, population and cases  based on location id
cluster_risk$clusterID <- 0 # create clusterID column

# Get a list of secondary clusters
secondary <- covid_kulldorff$secondary.clusters
id <- 1 # initialize clusterID

for (i in secondary) {
  secondary_temp <- covid_table[i$location.IDs.included, c(1, 2, 3)] 
  secondary_temp$clusterID <- id
  cluster_risk <- rbind(cluster_risk, secondary_temp) # appending to primary cluster dataframe
  id <- id + 1 # each cluster gets a new id
}

# Calculate total pop and cases for all counties in the US
total_pop <- sum(covid_table$pop) 
total_cases <- sum(covid_table$cases)

# Calculate and classify cluster-based relative risk
cluster_risk <- cluster_risk %>% 
  group_by(clusterID) %>%
  mutate(
    rr_cluster =
      (sum(cases) / sum(pop)) / ((total_cases - sum(cases)) / (total_pop - sum(pop))),
    cluster_class =
      (cut(rr_cluster, c(-Inf, 1, 2, 3, 4, 5, Inf), labels = FALSE))
  )

covid <- left_join(covid, cluster_risk
                   %>% dplyr::select(fips, clusterID, rr_cluster, cluster_class),by = "fips")

covid <- covid %>% 
  mutate(cluster_class = ifelse(is.na(cluster_class), 1, cluster_class))

```

Summarize the results of the Kulldorff spatial scan cluster detection by county Code each county `0` if it is not in a cluster and `1` if it is in a cluster.

```{r summarize Kulldorff results by county}

# Calculate and classify Local relative risk
# Counties outside of any cluster (with cluster risk of 1) are classified as lowest risk (1)
covid <- covid %>% 
  mutate( 
    rr_loc = 
      (cases / pop) / ((sum(covid$cases) - cases) / (sum(covid$pop) - pop)),
    loc_class = ifelse(cluster_class > 1, cut(rr_loc, c(-Inf,1,2,3,4,5,Inf), labels=FALSE), 1)
  ) 

```

### How did the classification work?

```{r classification results}

# Count frequency of each class of COVID risk
cat("Classes of local risk and frequency of counties",
  format(covid %>% st_drop_geometry %>% count(loc_class)), sep="\n")

cat("\n",
    sum(covid$cluster==0 & covid$rr_loc >= 1),
  " counties lie outside of a cluster, but have local relative risk > 1\n\n",
  sum(covid$cluster==1 & covid$rr_loc < 1),
  " counties lie inside of a cluster, but have a local relative risk < 1",
  sep="") 

# There's a relative risk score for both county & cluster (in SatScan)
# This reproduction uses county-based relative risk scores based on
# paragraph 4 of the methods section of the paper and Desjardins et al

```

```{r}
cat("Classes of cluster risk and frequency of counties",
  format(covid %>% st_drop_geometry %>% count(cluster_class)), sep="\n")
```

## Map Relative Risk Scores

Note that relative risk is \> 1 only if the county was in a cluster

```{r prepare original relative risk scores}

original_cluster <- read_sf(here("data", "derived", "public", "satscan", "sat_scan_compare.col.shp"))
cluster_table <- st_drop_geometry(original_cluster)

covid <- left_join(covid, cluster_table, by = c("fips" = "LOC_ID"))

covid_temp <- covid %>%
  mutate(rr_original = if_else(is.na(REL_RISK), 0, REL_RISK))

```

```{r map original relative risk scores}

tm4 <- tm_shape(covid_temp) +
  tm_polygons("rr_original",
    title = "Original Relative Risk",
    breaks = c(0, 1, 2, 3, 4, 5, 8),
    border.alpha = .2,
    lwd = 0.2,
    palette = "YlOrBr",
    labels = c("1", "2", "3", "4", "5", "6")
  ) +
  tm_shape(original_cluster) +
  tm_borders("red", lwd = .5) +
  tm_shape(state) +
  tm_borders("grey", lwd = .8) +
  tmap_options(check.and.fix = TRUE) + 
  tm_layout(
  legend.position = c("left", "bottom"),
  legend.title.size = 0.8,
  legend.text.size = 0.5)

tm4

tmap_save(tm4, here("results", "figures", "rr_original.png"))

```

```{r map cluster based relative risk scores}

tm5 <- tm_shape(covid) +
  tm_polygons("cluster_class",
    title = "Relative Risk by Cluster",
    border.alpha = .2,
    lwd = 0.2,
    palette = "YlOrBr",
    style = "cat"
  ) +
  tm_shape(state) +
  tm_borders("grey", lwd = .5) +
  tmap_options(check.and.fix = TRUE) +
  tm_layout(
  legend.position = c("left", "bottom"),
  legend.title.size = 0.8,
  legend.text.size = 0.5)
  

tm5

tmap_save(tm5, here("results", "figures", "rr_reproduction_cluster.png"))

```

```{r map local relative risk score}

# Map Local Relative Risk scores
tm3 <- tm_shape(covid) +
  tm_polygons("loc_class",
    title = "Local Relative Risk",
    border.alpha = .2,
    lwd = 0.2,
    palette = "YlOrBr",
    style = "cat"
  ) +
  tm_shape(state) +
  tm_borders("grey", lwd = .5) +
  tmap_options(check.and.fix = TRUE) +
  tm_layout(
  legend.position = c("left", "bottom"),
  legend.title.size = 0.8,
  legend.text.size = 0.5)

tm3
tmap_save(tm3, here("results", "figures", "rr_reproduction_loc.png"))


```

## Preprocess data for GEE modelling

This accomplishes the step 9 and 10 of the workflow diagram

```{r preprocess data for GEE model }

covid_clusters <- covid %>%
  select(fips, clusterID, rr_cluster, cluster_class) %>% # change to loc_class if calculate local rr
  st_drop_geometry()

# Filter out non-positive COVID rates and missing data
# Create unique State - Relative Risk IDs by combining state code and rr_class
# Sort by the cluster id's (a requirement of the gee function)
gee_data <- left_join(acs_covid, covid_clusters, by = c("geoid" = "fips")) %>%
  filter(covid_rate > 0) %>%
  mutate(id = as.integer(statefp) * 10 + cluster_class) %>%
  arrange(id)


gee_data <- gee_data %>%
  mutate(
    z_bpov_pct = scale(bpov_pct),
    z_apov_pct = scale(apov_pct),
    z_white_pct = scale(white_pct),
    z_black_pct = scale(black_pct),
    z_native_pct = scale(native_pct),
    z_asian_pct = scale(asian_pct),
    z_other_pct = scale(other_pct),
    z_non_hisp_white_pct = scale(non_hisp_white_pct),
    z_hisp_pct = scale(hisp_pct),
    z_non_hisp_non_white_pct = scale(non_hisp_non_white_pct),
    z_pct_5_17 = scale(pct_5_17),
    z_pct_18_34 = scale(pct_18_34),
    z_pct_35_64 = scale(pct_35_64),
    z_pct_65_74 = scale(pct_65_74),
    z_pct_75 = scale(pct_75),
    z_male_pct = scale(male_pct),
    z_female_pct = scale(female_pct)
  )


rm(covid_clusters)

```

## Save preprocessed GEE data inputs

Optionally, you may save the preprocessed to `data/raw/public/gee_data.gpkg`

```{r save preprocessed COVID cluster data, eval = FALSE}

write_sf(gee_data, here("data", "derived", "public", "gee_data.gpkg"))

```

## Load preprocessed GEE input data

Optionally, you may load the preprocessed data from `data/raw/public/gee_data.gpkg`

```{r load preprocessed COVID cluster data, eval = FALSE}

gee_data <- read_sf(here("data", "derived", "public", "gee_data.gpkg"))

```

# Report number of unique clusters and histogram of counties per cluster

```{r report unique clusters}

cluster_summary <- gee_data %>%
  st_drop_geometry() %>%
  count(id)
cat(length(cluster_summary$n), "unique clusters\n")
summary(cluster_summary$n)
hist(cluster_summary$n) # improve labels for this graph

```

## GEE Models

This accomplishes the step 11 of the workflow diagram

Generalized Estimating Equation parameters:

"The **'exchangeable' correlation matrix** was selected for the results reported here, since this specification yielded the best statistical fit based on the QIC (quasi- likelihood under the independence) model criterion." (Chakraborty 2021, Methods paragraph 5)

"The **gamma distribution** with **logarithmic link function** was chosen for all GEEs since this model specification provided the lowest QIC value." (Chakraborty 2021, Methods paragraph 5)

Useful Reference: <https://data.library.virginia.edu/getting-started-with-generalized-estimating-equations/>

```{r glm model}
race_glm <- glm(
  covid_rate ~ z_white_pct + z_black_pct + z_native_pct + z_asian_pct + z_other_pct,
  data = gee_data,
  family = Gamma(link = "log")
)

ethnicity_glm <- glm(
  covid_rate ~ z_non_hisp_white_pct + z_hisp_pct + z_non_hisp_non_white_pct,
  data = gee_data,
  family = Gamma(link = "log")
)

pov_glm <- glm(
  covid_rate ~ z_bpov_pct + z_apov_pct,
  data = gee_data,
  family = Gamma(link = "log")
)

age_glm <- glm(
  covid_rate ~ z_pct_5_17 + z_pct_18_34 + z_pct_35_64 + z_pct_65_74 + z_pct_75,
  data = gee_data,
  family = Gamma(link = "log")
)

sex_glm <- glm(
  covid_rate ~ z_male_pct + z_female_pct,
  data = gee_data,
  family = Gamma(link = "log")
)


glm_results <- rbind(
  coef(summary(race_glm)),
  coef(summary(ethnicity_glm)),
  coef(summary(pov_glm)),
  coef(summary(age_glm)),
  coef(summary(sex_glm))
) %>%
  round(3)


coefrows <- rownames(glm_results)
coefrows[1] <- "Race Intercept"
coefrows[7] <- "Ethnicity Intercept"
coefrows[11] <- "Poverty Status Intercept"
coefrows[14] <- "Age Intercept"
coefrows[20] <- "Biological Sex Intercept"
rownames(glm_results) <- coefrows
glm_results

```

```{r gee models}

# it would be smarter to iterate over a list of models and their parameters
# currently stuck on how to add GLM model results to a cell of a dataframe
# not the only one:
# https://www.reddit.com/r/rstats/comments/p50mce/coding_a_loop_for_many_linear_regressions/

race_gee <- geeglm(
  covid_rate ~ z_white_pct + z_black_pct + z_native_pct + z_asian_pct + z_other_pct,
  data = gee_data, # data frame
  id = id, # cluster IDs
  family = Gamma(link = "log"),
  corstr = "exchangeable"
)

# Wald and P calculated in summary only;
# coef() extracts coefficients table from the summary, same as $coefficients

ethnicity_gee <- geeglm(
  covid_rate ~ z_non_hisp_white_pct + z_hisp_pct + z_non_hisp_non_white_pct,
  data = gee_data,
  id = id,
  family = Gamma(link = "log"),
  corstr = "exchangeable"
)

pov_gee <- geeglm(
  covid_rate ~ z_bpov_pct + z_apov_pct,
  data = gee_data,
  id = id,
  family = Gamma(link = "log"),
  corstr = "exchangeable",
)

age_gee <- geeglm(
  covid_rate ~ z_pct_5_17 + z_pct_18_34 + z_pct_35_64 + z_pct_65_74 + z_pct_75,
  data = gee_data,
  id = id,
  family = Gamma(link = "log"),
  corstr = "exchangeable"
)

sex_gee <- geeglm(
  covid_rate ~ z_male_pct + z_female_pct,
  data = gee_data,
  id = id,
  family = Gamma(link = "log"),
  corstr = "exchangeable"
)

# summarize model coefficients
coefficient_results <- rbind(
  coef(summary(race_gee)),
  coef(summary(ethnicity_gee)),
  coef(summary(pov_gee)),
  coef(summary(age_gee)),
  coef(summary(sex_gee))
) %>%
  round(3)

# disambiguate intercepts
coefrows <- rownames(coefficient_results)
coefrows[1] <- "Race Intercept"
coefrows[7] <- "Ethnicity Intercept"
coefrows[11] <- "Poverty Status Intercept"
coefrows[14] <- "Age Intercept"
coefrows[20] <- "Biological Sex Intercept"
rownames(coefficient_results) <- coefrows
coefficient_results

# summarize model QICs
QIC_results <- data.frame(
  race = QIC(race_gee),
  ethnicity = QIC(ethnicity_gee),
  poverty_status = QIC(pov_gee),
  age = QIC(age_gee),
  biological_sex = QIC(sex_gee)
) %>%
  round(3) %>%
  t() %>%
  as.data.frame() %>%
  select(QIC)
QIC_results

```

```{r preprocess original gee data}
original_gee <- read.csv(here("data", "raw", "public", "chakraborty", "Aug1GEEdata.csv"))

original_processed <- original_gee %>%
  filter(Incidence > 0) %>%
  mutate(id = as.integer(COUNTY_FIPS) * 10 + RISK_BIN) %>%
  arrange(id)

```

```{r gee model with author provided data}

race_gee_original <- geeglm(
  Incidence ~ ZPD_White + ZPD_Black + ZPD_Asian + ZPD_Native + ZPD_OthRac,
  data = original_processed, # data frame
  id = id, # cluster IDs
  family = Gamma(link = "log"),
  corstr = "exchangeable"
)

ethnicity_gee_original <- geeglm(
  Incidence ~ ZPD_NHwhite + ZPD_Hispani + ZPD_NHoth,
  data = original_processed, # data frame
  id = id, # cluster IDs
  family = Gamma(link = "log"),
  corstr = "exchangeable"
)

pov_gee_original <- geeglm(
  Incidence ~ ZPDisBpov + ZPDisApov,
  data = original_processed, # data frame
  id = id, # cluster IDs
  family = Gamma(link = "log"),
  corstr = "exchangeable"
)

age_gee_original <- geeglm(
  Incidence ~ ZPD_age5to17 + ZPDage18to34 + ZPDage35to64 + ZPDage65to74 + ZPDage75,
  data = original_processed,
  id = id,
  family = Gamma(link = "log"),
  corstr = "exchangeable"
)

sex_gee_original <- geeglm(
  Incidence ~ ZPD_Male + ZPD_Female,
  data = original_processed, # data frame
  id = id, # cluster IDs
  family = Gamma(link = "log"),
  corstr = "exchangeable"
)

coefficient_results_original <- rbind(
  coef(summary(race_gee_original)),
  coef(summary(ethnicity_gee_original)),
  coef(summary(pov_gee_original)),
  coef(summary(age_gee_original)),
  coef(summary(sex_gee_original))
) %>%
  round(3)

coefrows <- rownames(coefficient_results_original)
coefrows[1] <- "Race Intercept"
coefrows[7] <- "Ethnicity Intercept"
coefrows[11] <- "Poverty Status Intercept"
coefrows[14] <- "Age Intercept"
coefrows[20] <- "Biological Sex Intercept"
rownames(coefficient_results_original) <- coefrows
coefficient_results_original


QIC_results_original <- data.frame(
  race = QIC(race_gee_original),
  ethnicity = QIC(ethnicity_gee_original),
  poverty_status = QIC(pov_gee_original),
  age = QIC(age_gee_original),
  biological_sex = QIC(sex_gee_original)
) %>%
  round(3) %>%
  t() %>%
  as.data.frame() %>%
  select(QIC)
QIC_results_original
```
